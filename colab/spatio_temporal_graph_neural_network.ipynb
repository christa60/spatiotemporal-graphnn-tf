{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spatio-temporal graph neural network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jApmuin_-3vB"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4lEScLlteID",
        "outputId": "f0b91d4c-8605-458b-c2d4-c8a815ed8b78"
      },
      "source": [
        "#In order to use sonnet 1, we need to downgrade a bunch of stuff\n",
        "#Using tensorflow-gpu>2 cause sonnet not finding the tensorflow-probability module. \n",
        "#There're some relative discussions online.\n",
        "#!pip install \"dm-sonnet<2\" \"tensorflow-probability==0.8.0\" \"tensorflow-gpu<2\"\n",
        "#The command cause following execution using tensorflow1\n",
        "!pip install \"dm-sonnet<2\" \"tensorflow-probability==0.8.0\" \"tensorflow-gpu<2\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dm-sonnet<2 in /usr/local/lib/python3.6/dist-packages (1.34)\n",
            "Requirement already satisfied: tensorflow-probability==0.8.0 in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: tensorflow-gpu<2 in /usr/local/lib/python3.6/dist-packages (1.15.4)\n",
            "Requirement already satisfied: semantic-version in /usr/local/lib/python3.6/dist-packages (from dm-sonnet<2) (2.8.5)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from dm-sonnet<2) (0.5.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from dm-sonnet<2) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from dm-sonnet<2) (1.15.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from dm-sonnet<2) (1.12.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0) (1.18.5)\n",
            "Requirement already satisfied: cloudpickle==1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0) (1.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0) (4.4.2)\n",
            "Requirement already satisfied: gast<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2) (1.15.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2) (3.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2) (1.33.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2) (0.35.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2) (3.12.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2) (3.3.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2) (50.3.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu<2) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu<2) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS7jYbUC1_v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea78e26a-fa27-4387-b1ca-4cce95db5b41"
      },
      "source": [
        "import tensorflow as tf\n",
        "import sonnet as snt\n",
        "\n",
        "print(\"TensorFlow version {}\".format(tf.__version__))\n",
        "print(\"Sonnet version {}\".format(snt.__version__))\n",
        "\n",
        "import collections\n",
        "import copy\n",
        "import random\n",
        "import time\n",
        "import itertools\n",
        "\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "tf.disable_eager_execution()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version 1.15.4\n",
            "Sonnet version 1.34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ym8s6r35Izy"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This colab builds a graph neural network that learns from the spatio-temporal signals (ST-GNN). \n",
        "\n",
        "* Model spec:\n",
        "  * Both the nodes and edges features are temporal, and encoder will perform sequence compression on the temporal information.\n",
        "  * Message passing unit in the ST-GNN can perform one way proprogation of neighorbing features together with the edge features.\n",
        "  * Then node level aggregation is done to get the final node representation that encodes both temporal and spatial information. \n",
        "  * The ST-GNN can be extended by making modifications on the encoder, message passing, aggregation modules. For examples, add attention layer to encoder, add attention in aggregation layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTzJrPy-hWez"
      },
      "source": [
        "# Build a ST-GNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00epZffw4_yE"
      },
      "source": [
        "## Graph encoder module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vDSo0qI5Btz"
      },
      "source": [
        "class GraphEncoder(snt.AbstractModule):\n",
        "  \"\"\"Module that encodes node and edge features to embedding vectors.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               node_hidden_sizes=None,\n",
        "               edge_hidden_sizes=None,\n",
        "               name='graph_encoder'):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      node_hidden_sizes: if provided should be a list of ints, hidden sizes of\n",
        "        node encoder network, the last element is the size of the node outputs.\n",
        "        If not provided, node features will pass through as is.\n",
        "      edge_hidden_sizes: if provided should be a list of ints, hidden sizes of\n",
        "        edge encoder network, the last element is the size of the edge outptus.\n",
        "        If not provided, edge features will pass through as is.\n",
        "      name: name of this module.\n",
        "    \"\"\"\n",
        "    super(GraphEncoder, self).__init__(name=name)\n",
        "    self._node_hidden_sizes = node_hidden_sizes\n",
        "    self._edge_hidden_sizes = edge_hidden_sizes\n",
        "\n",
        "  def _build(self, node_features, edge_features=None):\n",
        "    \"\"\"Encodes the node and edge features.\n",
        "\n",
        "    Args:\n",
        "      node_features: [n_nodes, node_seq_len, node_feat_dim] float tensor.\n",
        "      edge_features: if provided, should be \n",
        "        [n_edges, edge_seq_len, edge_feat_dim] float tensor.\n",
        "\n",
        "    Returns:\n",
        "      node_outputs: [n_nodes, node_embedding_dim] float tensor, node embeddings.\n",
        "      edge_outputs: if both edge_features and edge_hidden_sizes is provided,\n",
        "        this is [n_edges, edge_embedding_dim] float tensor, edge embeddings; \n",
        "        otherwise just the input edge_features.\n",
        "    \"\"\"\n",
        "    if self._node_hidden_sizes is None:\n",
        "      node_outputs = node_features\n",
        "    else:\n",
        "      nodel_lstm_layers = []\n",
        "      for hidden_size in self._node_hidden_sizes:\n",
        "        nodel_lstm_layers.append(snt.LSTM(hidden_size))\n",
        "      node_lstm = snt.DeepRNN(nodel_lstm_layers, \n",
        "                              skip_connections=True, \n",
        "                              name='node_deep_rnn')\n",
        "      node_features_t = tf.transpose(node_features, perm=[1, 0, 2]) \n",
        "      node_outputs, node_final_state = tf.nn.dynamic_rnn(\n",
        "          node_lstm, node_features_t, dtype=tf.float32)\n",
        "\n",
        "    if edge_features is None or self._edge_hidden_sizes is None:\n",
        "      edge_outputs = edge_features\n",
        "    else:\n",
        "      edge_lstm_layers = []\n",
        "      for hidden_size in self._edge_hidden_sizes:\n",
        "        edge_lstm_layers.append(snt.LSTM(hidden_size))\n",
        "      edge_lstm = snt.DeepRNN(edge_lstm_layers, \n",
        "                              skip_connections=True, name='edge_deep_rnn')      \n",
        "      edge_features_t = tf.transpose(edge_features, perm=[1, 0, 2]) \n",
        "      edge_outputs, edge_final_state = tf.nn.dynamic_rnn(\n",
        "          edge_lstm, edge_features_t, dtype=tf.float32) \n",
        "\n",
        "    # Use the last hiddent state  \n",
        "    return node_outputs[-1], edge_outputs[-1]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnAS63DI5Dxd"
      },
      "source": [
        "## Graph message passing module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyWFQeVehmLw"
      },
      "source": [
        "def graph_prop_once(node_states,\n",
        "                    from_idx,\n",
        "                    to_idx,\n",
        "                    message_net,\n",
        "                    aggregation_module=tf.math.unsorted_segment_mean,\n",
        "                    edge_features=None):\n",
        "  \"\"\"Performs one round of propagation (message passing) in a graph. \n",
        "    Updates the messags(at edge level) and aggregate the messages at node level.\n",
        "\n",
        "  Args:\n",
        "    node_states: [n_nodes, node_state_dim] float tensor, one row for each node.\n",
        "    from_idx: [n_edges] int tensor, index of the from nodes.\n",
        "    to_idx: [n_edges] int tensor, index of the to nodes.\n",
        "    message_net: a network that maps concatenated edge inputs to message vectors.\n",
        "    aggregation_module: a module that aggregates messages on edges to aggregated\n",
        "      messages for each node. Should be a callable and can be called like the\n",
        "      following,\n",
        "        `aggregated_messages = aggregation_module(messages, to_idx, n_nodes)`,\n",
        "      where messages is [n_edges, edge_message_dim] tensor, to_idx is the index\n",
        "      of the to_nodes, i.e. where each message goes to, and n_nodes is an\n",
        "      int which is the number of nodes to aggregate into.\n",
        "    edge_features: if provided, should be a [n_edges, edge_feature_dim] float\n",
        "      tensor, extra features for each edge.\n",
        "\n",
        "  Returns:\n",
        "    aggregated_messages: [n_nodes, edge_message_dim] float tensor, the\n",
        "      aggregated messages, one row for each node.\n",
        "  \"\"\"\n",
        "  from_states = tf.gather(node_states, from_idx)\n",
        "  to_states = tf.gather(node_states, to_idx)\n",
        "\n",
        "  edge_inputs = [from_states, to_states]\n",
        "  if edge_features is not None:\n",
        "    edge_inputs.append(edge_features)\n",
        "\n",
        "  edge_inputs = tf.concat(edge_inputs, axis=-1)\n",
        "  messages = message_net(edge_inputs)\n",
        "\n",
        "  return aggregation_module(messages, to_idx, tf.shape(node_states)[0]), messages\n",
        "\n",
        "\n",
        "class GraphPropLayer(snt.AbstractModule):\n",
        "  \"\"\"Graph propagation (or message passing) module.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               node_state_dim,\n",
        "               edge_hidden_sizes,\n",
        "               node_hidden_sizes,\n",
        "               edge_net_init_scale=0.1,\n",
        "               node_update_type='gru',\n",
        "               use_reverse_direction=False,\n",
        "               reverse_dir_param_different=True,\n",
        "               aggregation_module=tf.math.unsorted_segment_mean,\n",
        "               layer_norm=True,\n",
        "               name='graph_mp'):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      node_state_dim: int, the dimension of node states.\n",
        "      edge_hidden_sizes: list of ints, hidden sizes for the edge message\n",
        "        net, the last element in the list is the size of the message vectors.\n",
        "      node_hidden_sizes: list of ints, hidden sizes for the node update\n",
        "        net.\n",
        "      edge_net_init_scale: initialization scale for the edge networks.  This\n",
        "        is typically set to a small value such that the gradient does not blow\n",
        "        up.\n",
        "      node_update_type: type of node updates, one of {mlp, gru, residual}.\n",
        "      use_reverse_direction: set to True to also propagate messages in the\n",
        "        reverse direction.\n",
        "      reverse_dir_param_different: set to True to have the messages computed\n",
        "        using a different set of parameters than for the forward direction.\n",
        "      layer_norm: set to True to use layer normalization.\n",
        "      name: name of this module.\n",
        "    \"\"\"\n",
        "    super(GraphPropLayer, self).__init__(name=name)\n",
        "\n",
        "    self._node_state_dim = node_state_dim\n",
        "    self._edge_hidden_sizes = edge_hidden_sizes[:]\n",
        "\n",
        "    # output size is node_state_dim\n",
        "    self._node_hidden_sizes = node_hidden_sizes[:] + [node_state_dim]\n",
        "    self._edge_net_init_scale = edge_net_init_scale\n",
        "    self._node_update_type = node_update_type\n",
        "    self._aggregation_module = aggregation_module\n",
        "\n",
        "    self._use_reverse_direction = use_reverse_direction\n",
        "    self._reverse_dir_param_different = reverse_dir_param_different\n",
        "\n",
        "    self._layer_norm = layer_norm\n",
        "\n",
        "  def _compute_aggregated_messages(\n",
        "      self, node_states, from_idx, to_idx, edge_features=None):\n",
        "    \"\"\"Computes aggregated messages for each node.\n",
        "\n",
        "    Args:\n",
        "      node_states: [n_nodes, input_node_state_dim] float tensor, node states.\n",
        "      from_idx: [n_edges] int tensor, from node indices for each edge.\n",
        "      to_idx: [n_edges] int tensor, to node indices for each edge.\n",
        "      edge_features: if not None, should be [n_edges, edge_embedding_dim]\n",
        "        tensor, edge features.\n",
        "\n",
        "    Returns:\n",
        "      aggregated_messages: [n_nodes, aggregated_message_dim] float tensor, the\n",
        "        aggregated messages for each node.\n",
        "    \"\"\"\n",
        "    self._message_net = snt.nets.MLP(\n",
        "        self._edge_hidden_sizes,\n",
        "        initializers={\n",
        "            'w': tf.variance_scaling_initializer(\n",
        "                scale=self._edge_net_init_scale),\n",
        "            'b': tf.zeros_initializer()},\n",
        "        name='message_mlp')\n",
        "\n",
        "    aggregated_messages, new_edge_features = graph_prop_once(\n",
        "        node_states,\n",
        "        from_idx,\n",
        "        to_idx,\n",
        "        self._message_net,\n",
        "        aggregation_module=self._aggregation_module,\n",
        "        edge_features=edge_features)\n",
        "\n",
        "    # optionally compute message vectors in the reverse direction\n",
        "    if self._use_reverse_direction:\n",
        "      if self._reverse_dir_param_different:\n",
        "        self._reverse_message_net = snt.nets.MLP(\n",
        "            self._edge_hidden_sizes,\n",
        "            initializers={\n",
        "                'w': tf.variance_scaling_initializer(\n",
        "                    scale=self._edge_net_init_scale),\n",
        "                'b': tf.zeros_initializer()},\n",
        "            name='reverse_message_mlp')\n",
        "      else:\n",
        "        self._reverse_message_net = self._message_net\n",
        "\n",
        "      reverse_aggregated_messages = graph_prop_once(\n",
        "          node_states,\n",
        "          to_idx,\n",
        "          from_idx,\n",
        "          self._reverse_message_net,\n",
        "          aggregation_module=self._aggregation_module,\n",
        "          edge_features=edge_features)\n",
        "\n",
        "      aggregated_messages += reverse_aggregated_messages\n",
        "\n",
        "    if self._layer_norm:\n",
        "      aggregated_messages = snt.LayerNorm()(aggregated_messages)\n",
        "\n",
        "    return aggregated_messages, new_edge_features\n",
        "\n",
        "\n",
        "  def _compute_node_update(self,\n",
        "                           node_states,\n",
        "                           node_state_inputs,\n",
        "                           node_features=None):\n",
        "    \"\"\"Computes node updates.\n",
        "\n",
        "    Args:\n",
        "      node_states: [n_nodes, node_state_dim] float tensor, input node states.\n",
        "      node_state_inputs: list of tensors used to compute node updates.  Each\n",
        "        element tensor should have shape [n_nodes, feat_dim], where feat_dim can\n",
        "        be different from node_states. These tensors will be concatenated along\n",
        "        the feature dimension.\n",
        "      node_features: extra node features if provided, should be of size\n",
        "        [n_nodes, extra_node_feat_dim] float tensor.\n",
        "\n",
        "    Returns:\n",
        "      updated_node_states: [n_nodes, node_state_dim] float tensor, the new node\n",
        "        state tensor.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: if node update type is not supported.\n",
        "    \"\"\"\n",
        "    if self._node_update_type in ('mlp', 'residual'):\n",
        "      node_state_inputs.append(node_states)\n",
        "      \n",
        "    if node_features is not None:\n",
        "      node_state_inputs.append(node_features)\n",
        "\n",
        "    if len(node_state_inputs) == 1:\n",
        "      node_state_inputs = node_state_inputs[0]\n",
        "    else:\n",
        "      node_state_inputs = tf.concat(node_state_inputs, axis=-1)\n",
        "\n",
        "    if self._node_update_type == 'gru':\n",
        "      _, new_node_states = snt.GRU(self._node_state_dim)(\n",
        "          node_state_inputs, node_states)\n",
        "      return new_node_states\n",
        "    else:\n",
        "      mlp_output = snt.nets.MLP(\n",
        "          self._node_hidden_sizes, name='node_mlp')(node_state_inputs)\n",
        "      if self._layer_norm:\n",
        "        mlp_output = snt.LayerNorm()(mlp_output)\n",
        "      if self._node_update_type == 'mlp':\n",
        "        return mlp_output\n",
        "      elif self._node_update_type == 'residual':\n",
        "        return node_states + mlp_output\n",
        "      else:\n",
        "        raise ValueError('node update type {} not supported'.format(self._node_update_type))\n",
        "\n",
        "  def _build(self,\n",
        "             node_states,\n",
        "             from_idx,\n",
        "             to_idx,\n",
        "             edge_features=None,\n",
        "             node_features=None):\n",
        "    \"\"\"Runs one round of propagation (message passing).\n",
        "\n",
        "    Args:\n",
        "      node_states: [n_nodes, input_node_state_dim] float tensor, node states.\n",
        "      from_idx: [n_edges] int tensor, from node indices for each edge.\n",
        "      to_idx: [n_edges] int tensor, to node indices for each edge.\n",
        "      edge_features: if not None, should be [n_edges, edge_embedding_dim]\n",
        "        tensor, edge features.\n",
        "      node_features: extra node features if provided, should be of size\n",
        "        [n_nodes, extra_node_feat_dim] float tensor, can be used to implement\n",
        "        different types of skip connections.\n",
        "\n",
        "    Returns:\n",
        "      node_states: [n_nodes, node_state_dim] float tensor, new node states.\n",
        "    \"\"\"\n",
        "    aggregated_messages, updated_edge_features = self._compute_aggregated_messages(\n",
        "        node_states, from_idx, to_idx, edge_features=edge_features)\n",
        "    \n",
        "    # node_states: initial node feature for current round of message passing\n",
        "    # aggregated_messages: messages from the message passing proporgation\n",
        "    return self._compute_node_update(node_states,\n",
        "                                     [aggregated_messages],\n",
        "                                     node_features=node_features), updated_edge_features"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY6-3nEJ5Ib_"
      },
      "source": [
        "## Graph output module\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKIUQPL8lw2A"
      },
      "source": [
        "class GraphOutput(snt.AbstractModule):\n",
        "  \"\"\"Computes node representations and make node level predictions.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               node_hidden_sizes,\n",
        "               output_dim=1,\n",
        "               gated=True,\n",
        "               aggregation_type='mean',\n",
        "               name='graph_aggregator'):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      node_hidden_sizes: the hidden layer sizes of the node transformation nets.\n",
        "        The last element is the size of the aggregated graph representation.\n",
        "      output_dim: int. Output dimension for each node.\n",
        "      gated: set to True to do gated aggregation, False not to.\n",
        "      aggregation_type: one of {sum, max, mean, sqrt_n}.\n",
        "      name: name of this module.\n",
        "    \"\"\"\n",
        "    super(GraphOutput, self).__init__(name=name)\n",
        "\n",
        "    self._node_hidden_sizes = node_hidden_sizes\n",
        "    self._graph_state_dim = node_hidden_sizes[-1]\n",
        "    self._gated = gated\n",
        "    self._output_dim = output_dim\n",
        "\n",
        "  def _build(self, node_states, graph_idx, n_graphs):\n",
        "    \"\"\"Compute node representation and level predictions.\n",
        "\n",
        "    Args:\n",
        "      node_states: [n_nodes, node_state_dim] float tensor, node states of a\n",
        "        batch of graphs concatenated together along the first dimension.\n",
        "      graph_idx: [n_nodes] int tensor, graph id for each node.\n",
        "      n_graphs: integer, number of graphs in this batch.\n",
        "\n",
        "    Returns:\n",
        "      node_states: [n_nodes, node_state_dim] float tensor, the node level \n",
        "        prediction.\n",
        "    \"\"\"\n",
        "    node_hidden_sizes = self._node_hidden_sizes\n",
        "    if self._gated:\n",
        "      node_hidden_sizes[-1] = self._graph_state_dim * 2\n",
        "\n",
        "    node_states_g = snt.nets.MLP(\n",
        "        node_hidden_sizes, name='node_state_g_mlp')(node_states)\n",
        "\n",
        "    if self._gated:\n",
        "      gates = tf.nn.sigmoid(node_states_g[:, :self._graph_state_dim])\n",
        "      node_states_g = node_states_g[:, self._graph_state_dim:] * gates \n",
        "\n",
        "    # Transforms the node states for final node level output\n",
        "    node_states_output = snt.nets.MLP(\n",
        "        [self._output_dim], name='node_state_transform_mlp')(node_states_g) \n",
        "    \n",
        "    return node_states_output    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHhUx0gq7Kum"
      },
      "source": [
        "## Graph model\n",
        "\n",
        "Now we can put the three modules all together with encoder, message-passing, graph aggregation modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpB4Y4ZA7Mj0"
      },
      "source": [
        "AGGREGATION_TYPE = {\n",
        "    'sum': tf.math.unsorted_segment_sum,\n",
        "    'mean': tf.math.unsorted_segment_mean,\n",
        "    'sqrt_n': tf.math.unsorted_segment_sqrt_n,\n",
        "    'max': tf.math.unsorted_segment_max,\n",
        "}\n",
        "\n",
        "class GraphSTNet(snt.AbstractModule):\n",
        "  \"\"\"A ST-GNN to learn both spatial and temporal interactions in the network.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               encoder,\n",
        "               output_module,\n",
        "               node_state_dim,\n",
        "               edge_hidden_sizes,\n",
        "               node_hidden_sizes,\n",
        "               n_prop_layers,\n",
        "               label_seq_len=7,\n",
        "               label_dim=1,\n",
        "               share_prop_params=False,\n",
        "               edge_net_init_scale=0.1,\n",
        "               node_update_type='residual',\n",
        "               use_reverse_direction=True,\n",
        "               reverse_dir_param_different=True,\n",
        "               layer_norm=False,\n",
        "               aggregation_type='mean',\n",
        "               name='graph_st_net'):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      encoder: GraphEncoder, encoder that maps features to embeddings.\n",
        "      aggregator: GraphAggregator, aggregator that produces node level \n",
        "        predictions.\n",
        "      node_state_dim: dimensions of node states.\n",
        "      edge_hidden_sizes: sizes of the hidden layers of the edge message nets.\n",
        "      node_hidden_sizes: sizes of the hidden layers of the node update nets.\n",
        "      n_prop_layers: number of graph propagation layers.\n",
        "      share_prop_params: set to True to share propagation parameters across all\n",
        "        graph propagation layers, False not to.\n",
        "      edge_net_init_scale: scale of initialization for the edge message nets.\n",
        "      node_update_type: type of node updates, one of {mlp, gru, residual}.\n",
        "      use_reverse_direction: set to True to also propagate messages in the\n",
        "        reverse direction.\n",
        "      reverse_dir_param_different: set to True to have the messages computed\n",
        "        using a different set of parameters than for the forward direction.\n",
        "      layer_norm: set to True to use layer normalization in a few places.\n",
        "      name: name of this module.\n",
        "    \"\"\"\n",
        "    super(GraphSTNet, self).__init__(name=name)\n",
        "\n",
        "    self._encoder = encoder\n",
        "    self._output = output_module\n",
        "    self._node_state_dim = node_state_dim\n",
        "    self._edge_hidden_sizes = edge_hidden_sizes\n",
        "    self._node_hidden_sizes = node_hidden_sizes\n",
        "    self._n_prop_layers = n_prop_layers\n",
        "    self._share_prop_params = share_prop_params\n",
        "    self._edge_net_init_scale = edge_net_init_scale\n",
        "    self._node_update_type = node_update_type\n",
        "    self._use_reverse_direction = use_reverse_direction\n",
        "    self._reverse_dir_param_different = reverse_dir_param_different\n",
        "    self._layer_norm = layer_norm\n",
        "    self._aggregation_module = AGGREGATION_TYPE[aggregation_type]\n",
        "\n",
        "    self._prop_layers = []\n",
        "    self._layer_class = GraphPropLayer\n",
        "\n",
        "    self._label_seq_len = label_seq_len\n",
        "    self._label_dim = label_dim\n",
        "\n",
        "\n",
        "  def _build_layer(self, layer_id):\n",
        "    \"\"\"Build one layer in the network.\"\"\"\n",
        "    return self._layer_class(\n",
        "        self._node_state_dim,\n",
        "        self._edge_hidden_sizes,\n",
        "        self._node_hidden_sizes,\n",
        "        edge_net_init_scale=self._edge_net_init_scale,\n",
        "        node_update_type=self._node_update_type,\n",
        "        use_reverse_direction=self._use_reverse_direction,\n",
        "        reverse_dir_param_different=self._reverse_dir_param_different,\n",
        "        layer_norm=self._layer_norm,\n",
        "        aggregation_module=self._aggregation_module,\n",
        "        name='graph_prop_%d' % layer_id)\n",
        "\n",
        "  def _apply_layer(self,\n",
        "                   layer,\n",
        "                   node_states,\n",
        "                   from_idx,\n",
        "                   to_idx,\n",
        "                   graph_idx,\n",
        "                   n_graphs,\n",
        "                   edge_features):\n",
        "    \"\"\"Apply one layer on the given inputs.\"\"\"\n",
        "    del graph_idx, n_graphs\n",
        "    return layer(node_states, from_idx, to_idx, edge_features=edge_features)\n",
        "\n",
        "\n",
        "  def _build(self,\n",
        "             node_features,\n",
        "             edge_features,\n",
        "             from_idx,\n",
        "             to_idx,\n",
        "             graph_idx,\n",
        "             n_graphs):\n",
        "    \"\"\"Compute node prediction.\n",
        "\n",
        "    Args:\n",
        "      node_features: [n_nodes, node_feat_dim] float tensor.\n",
        "      edge_features: [n_edges, edge_feat_dim] float tensor.\n",
        "      from_idx: [n_edges] int tensor, index of the from node for each edge.\n",
        "      to_idx: [n_edges] int tensor, index of the to node for each edge.\n",
        "      graph_idx: [n_nodes] int tensor, graph id for each node.\n",
        "      n_graphs: int, number of graphs in the batch.\n",
        "\n",
        "    Returns:\n",
        "      node_prediction: [n_nodes, label_dim] float tensor.\n",
        "    \"\"\"\n",
        "    if len(self._prop_layers) < self._n_prop_layers:\n",
        "      # build the layers\n",
        "      for i in range(self._n_prop_layers):\n",
        "        if i == 0 or not self._share_prop_params:\n",
        "          layer = self._build_layer(i)\n",
        "        else:\n",
        "          layer = self._prop_layers[0]\n",
        "        self._prop_layers.append(layer)\n",
        "\n",
        "    node_states, edge_states = self._encoder(node_features, edge_features)\n",
        "\n",
        "    layer_outputs = [node_states]\n",
        "    curre_edge_states = edge_states\n",
        "    for layer in self._prop_layers:\n",
        "      node_states, new_edge_states = self._apply_layer(\n",
        "          layer,\n",
        "          node_states,\n",
        "          from_idx,\n",
        "          to_idx,\n",
        "          graph_idx,\n",
        "          n_graphs,\n",
        "          curre_edge_states)\n",
        "      curre_edge_states = new_edge_states\n",
        "      layer_outputs.append(node_states)\n",
        "\n",
        "    # these tensors may be used e.g. for visualization\n",
        "    self._layer_outputs = layer_outputs\n",
        "\n",
        "    # predictions at each node level, node*(Pred_len)\n",
        "    node_pred = self._output(node_states, graph_idx, n_graphs)  \n",
        "    return node_pred\n",
        "\n",
        "  def reset_n_prop_layers(self, n_prop_layers):\n",
        "    \"\"\"Sets n_prop_layers to the provided new value.\n",
        "\n",
        "    This allows us to train with certain number of propagation layers and\n",
        "    evaluate with a different number of propagation layers.\n",
        "\n",
        "    This only works if n_prop_layers is smaller than the number used for\n",
        "    training, or when share_prop_params is set to True, in which case this can\n",
        "    be arbitrarily large.\n",
        "\n",
        "    Args:\n",
        "      n_prop_layers: the new number of propagation layers to set.\n",
        "    \"\"\"\n",
        "    self._n_prop_layers = n_prop_layers\n",
        "\n",
        "  @property\n",
        "  def n_prop_layers(self):\n",
        "    return self._n_prop_layers\n",
        "\n",
        "  def get_layer_outputs(self):\n",
        "    \"\"\"Get the outputs at each layer.\"\"\"\n",
        "    if hasattr(self, '_layer_outputs'):\n",
        "      return self._layer_outputs\n",
        "    else:\n",
        "      raise ValueError('No layer outputs available.')    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dozKgDg1ZHTy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a89afb7f-0477-44e4-d999-792f014557d9"
      },
      "source": [
        "mymodel = GraphSTNet(GraphEncoder([8], [8]),\n",
        "                     GraphOutput([8]),\n",
        "                     8, [8], [8], 2 )\n",
        "mymodel"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/sonnet/python/modules/base.py:177: The name tf.make_template is deprecated. Please use tf.compat.v1.make_template instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.GraphSTNet at 0x7f2b539f91d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1S9ekW-_hyH"
      },
      "source": [
        "## Define loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opXymDuZ_jaY"
      },
      "source": [
        "def compute_loss(y, yhat, loss_type='mse'):\n",
        "  # yhat: node_num*seq_len*label_dim\n",
        "  if loss_type == 'mse':\n",
        "    d_case_loss = tf.keras.losses.MSE(y, yhat[:,:,0])   \n",
        "    return tf.reduce_mean(d_case_loss)       \n",
        "  else:\n",
        "    raise ValueError('Unknown loss_type %s' % loss_type)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEQlGa90xkd4"
      },
      "source": [
        "#### Test the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJu3xXw-oXUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87b41cf2-1efd-451e-96ec-c410986d5f32"
      },
      "source": [
        "# Test loss function\n",
        "n_nodes = 5\n",
        "y = np.random.normal(size=[n_nodes,  7])\n",
        "yhat = np.random.normal(size=[n_nodes, 7, 3])\n",
        "\n",
        "total_loss = compute_loss(y, yhat, loss_type='mse')\n",
        "sess = tf.Session()\n",
        "sess.run(total_loss)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5563361457923817"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDTJbumUEUTS"
      },
      "source": [
        "# Graph Datasets\n",
        "To represent a graph, we need node level and edge level information, from_idx, and to_idx to describe the connectivity in the graph. Because training and evaluation is usually done in batches, data from multiple graphs are concatinated together, therefore, we also need describe the total number of graphs and the index of graph that each node belongs to.\n",
        "\n",
        "The following data structure captures what we have discussed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QiPUT3Wh1B0"
      },
      "source": [
        "list_data_info = ['from_idx',\n",
        "                  'to_idx',\n",
        "                  'node_features',\n",
        "                  'edge_features',\n",
        "                  'graph_idx',\n",
        "                  'n_graphs',\n",
        "                  'node_labels']\n",
        "GraphData = collections.namedtuple('GraphData', list_data_info)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV8rz_zdVbHC"
      },
      "source": [
        "## Simulated dataset \n",
        "We would like our graph data to capture both spatial and temporal information. The temporal information is wrapped in node feature and edge features; the spatial interaction is encoded naturally in the graph structure. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4lsV6yNhWB-"
      },
      "source": [
        "class GraphDataset():\n",
        "  \"\"\"Graph dataset for node level predictions.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               n_nodes_range,\n",
        "               n_edge_range):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      n_nodes_range: a tuple (n_min, n_max).  The minimum and maximum number of\n",
        "        nodes in a graph to generate.\n",
        "      p_edge_range: a tuple (p_min, p_max).  The minimum and maximum edge\n",
        "        probability.\n",
        "      n_changes_positive: the number of edge substitutions for a pair to be\n",
        "        considered positive (similar).\n",
        "      n_changes_negative: the number of edge substitutions for a pair to be\n",
        "        considered negative (not similar).\n",
        "      permute: if True (default), permute node orderings in addition to\n",
        "        changing edges; if False, the node orderings across a pair or triplet of\n",
        "        graphs will be the same, useful for visualization.\n",
        "    \"\"\"\n",
        "    self._n_min, self._n_max = n_nodes_range\n",
        "    self._e_min, self._e_max = n_edge_range\n",
        "\n",
        "  def _get_graph(self):\n",
        "    \"\"\"Generates one graph.\"\"\"\n",
        "    seq_len, node_feat_dim, edge_feat_dim = 7, 3, 2\n",
        "    n_nodes = np.random.randint(self._n_min, self._n_max + 1)\n",
        "    n_edges = np.random.randint(self._e_min, self._e_max+1)\n",
        "    nodes = np.random.normal(size=[n_nodes,  seq_len, node_feat_dim])\n",
        "    edges = np.random.normal(size=[n_edges, seq_len, edge_feat_dim])\n",
        "    senders = np.random.randint(0, n_nodes, size=[n_edges]) \n",
        "    receivers = np.random.randint(0, n_nodes, size=[n_edges]) \n",
        "\n",
        "    # Assume the label dim is 1, which is usually the case.\n",
        "    node_labels = np.random.normal(size=[n_nodes, 1])\n",
        "\n",
        "    return GraphData(from_idx=senders,\n",
        "                     to_idx=receivers,\n",
        "                     node_features=nodes,\n",
        "                     edge_features=edges,\n",
        "                     node_labels=node_labels,\n",
        "                     graph_idx=np.zeros(n_nodes, dtype=np.int32),\n",
        "                     n_graphs=1)\n",
        "\n",
        "\n",
        "  def _get_one_batch(self, batch_size):\n",
        "    \"\"\"Build a batch of GraphData objects into one GraphData.\n",
        "    \n",
        "    This is done by concatinating the different data in each GraphData in the \n",
        "    first dimension. Graph index needs to be updated dynamically to distinguish\n",
        "    between different graphs.\n",
        "    \"\"\"\n",
        "    from_idx = []\n",
        "    to_idx = []\n",
        "    graph_idx = []\n",
        "    node_feat = []\n",
        "    edge_feat = []\n",
        "    node_label =[]\n",
        "\n",
        "    n_total_nodes = 0\n",
        "    n_total_edges = 0\n",
        "    for i in range(batch_size):\n",
        "      g = self._get_graph()\n",
        "      n_nodes = g.node_features.shape[0]\n",
        "      n_edges = g.edge_features.shape[0]\n",
        "\n",
        "      # Update the node indices for the edges\n",
        "      from_idx.append(g.from_idx + n_total_nodes)\n",
        "      to_idx.append(g.to_idx + n_total_nodes)\n",
        "      graph_idx.append(np.ones(n_nodes, dtype=np.int32) * i)\n",
        "      node_feat.append(g.node_features)\n",
        "      edge_feat.append(g.edge_features)\n",
        "      node_label.append(g.node_labels)\n",
        "\n",
        "      n_total_nodes += n_nodes\n",
        "      n_total_edges += n_edges\n",
        "\n",
        "    return GraphData(\n",
        "        from_idx=np.concatenate(from_idx, axis=0),\n",
        "        to_idx=np.concatenate(to_idx, axis=0),\n",
        "        node_features=np.concatenate(node_feat, axis=0),\n",
        "        edge_features=np.concatenate(edge_feat, axis=0),\n",
        "        graph_idx=np.concatenate(graph_idx, axis=0),\n",
        "        node_labels=np.concatenate(node_label, axis=0),\n",
        "        n_graphs=batch_size)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm7KYI3plGtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15244cde-d928-4284-9d5b-eb0d0a1351a1"
      },
      "source": [
        "# Test the dataset generation.\n",
        "graph_gen = GraphDataset((3, 3), (2, 2))\n",
        "graph_gen._get_one_batch(3) # this looks good"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GraphData(from_idx=array([2, 0, 5, 3, 7, 7]), to_idx=array([2, 0, 5, 3, 6, 8]), node_features=array([[[ 6.49750554e-01, -5.28160264e-01, -2.73474921e-01],\n",
              "        [ 2.27882079e+00, -7.55231331e-01, -1.01548164e+00],\n",
              "        [-2.29521214e-01, -4.74420000e-01, -5.42315706e-01],\n",
              "        [-1.64376636e+00, -4.75936718e-01, -2.45602534e-01],\n",
              "        [ 1.26626501e+00,  3.32855644e-02,  6.18323443e-02],\n",
              "        [ 8.73964463e-01, -1.05059542e+00,  4.99566536e-02],\n",
              "        [-5.01248912e-01, -1.17138958e+00, -1.71597137e+00]],\n",
              "\n",
              "       [[-4.79838393e-02, -6.99212662e-02, -1.55647892e+00],\n",
              "        [ 4.93130044e-01,  7.92857523e-01, -2.61839908e-01],\n",
              "        [-1.46919304e+00,  1.07571456e+00, -3.57115584e-01],\n",
              "        [ 5.34141818e-01, -7.24781613e-01, -1.02628521e+00],\n",
              "        [ 2.08674819e+00, -1.84461728e+00,  3.29698717e-02],\n",
              "        [-1.31372144e+00,  2.99319106e-01,  1.20925408e+00],\n",
              "        [ 2.07811860e+00, -3.21434481e+00, -1.22254428e+00]],\n",
              "\n",
              "       [[ 8.86901997e-02, -1.67209707e-01,  1.19058455e+00],\n",
              "        [ 5.09585688e-01,  5.10034370e-01, -8.99407170e-01],\n",
              "        [ 1.28404630e+00,  7.95886858e-01,  7.42813834e-01],\n",
              "        [-2.47623217e+00,  3.88120072e-01, -9.44930254e-01],\n",
              "        [ 2.10960571e-01, -1.84510338e+00, -5.94629461e-01],\n",
              "        [-9.18169981e-01, -2.77299372e-01, -5.66547074e-02],\n",
              "        [ 2.16893270e-03,  3.54269361e-01, -1.56027975e+00]],\n",
              "\n",
              "       [[ 1.49504063e-01,  9.51722936e-01,  1.52519432e+00],\n",
              "        [ 2.21677134e+00,  6.02739022e-01, -5.89759933e-01],\n",
              "        [-2.75651805e-01,  4.08215791e-01,  1.66301934e-01],\n",
              "        [-1.02723327e+00,  1.85297353e+00, -3.75443665e-01],\n",
              "        [ 1.20877838e+00, -1.72096277e+00, -1.60667199e+00],\n",
              "        [ 1.20741944e+00,  1.10387529e+00, -7.78046132e-01],\n",
              "        [-1.29507896e-01, -4.27311000e-01,  1.17376004e+00]],\n",
              "\n",
              "       [[-1.13420097e+00,  1.07874210e+00,  6.81359236e-01],\n",
              "        [-3.46168161e-01,  5.55985590e-01, -3.78721865e-01],\n",
              "        [ 9.12945016e-01,  4.08447380e-01,  2.25334359e-01],\n",
              "        [ 6.91698009e-01, -7.07726643e-01, -6.36717652e-01],\n",
              "        [-1.58823048e+00, -2.03683922e-01, -1.86098907e-01],\n",
              "        [-6.38161260e-01, -1.53201541e+00,  2.36211798e+00],\n",
              "        [ 4.78334004e-02, -9.00200807e-01, -5.16252986e-01]],\n",
              "\n",
              "       [[-1.07837934e+00, -7.29969913e-01,  1.18014225e+00],\n",
              "        [-4.37650290e-01, -8.88386245e-01, -2.40449011e+00],\n",
              "        [-1.10217443e+00,  3.65558111e-02, -1.81256658e-01],\n",
              "        [ 1.08136958e-01, -1.06515489e+00,  1.40446324e-02],\n",
              "        [ 2.89467594e-01,  3.86350228e-01, -2.62209153e+00],\n",
              "        [ 1.29591874e+00,  1.49879937e+00,  1.89643742e-01],\n",
              "        [ 1.75266318e+00,  1.20119145e-01, -1.79772826e-02]],\n",
              "\n",
              "       [[-4.35420787e-01, -2.82722929e-01, -2.07329597e-01],\n",
              "        [-4.92247873e-01,  4.35317649e-01,  2.64144339e-01],\n",
              "        [-2.28004521e-01,  5.31875923e-01,  5.92136021e-01],\n",
              "        [ 3.86834815e-01, -7.34435585e-01, -1.40333992e+00],\n",
              "        [ 4.66798410e-01,  5.33094384e-01, -1.84389748e+00],\n",
              "        [ 1.48727334e+00, -5.00817020e-01, -4.59459568e-01],\n",
              "        [-8.83513949e-02, -3.32245411e-01, -2.17506193e-01]],\n",
              "\n",
              "       [[ 1.29303900e-01,  1.26452236e+00, -1.18487348e+00],\n",
              "        [-8.29741369e-02,  1.52250196e+00, -9.24189075e-01],\n",
              "        [ 2.13701022e+00, -4.75495747e-01,  2.08641088e+00],\n",
              "        [ 4.76662458e-01, -8.11577984e-01, -1.05181107e+00],\n",
              "        [ 1.79928550e+00,  1.36654805e-01, -8.03984537e-01],\n",
              "        [-9.21997633e-01, -1.24352876e+00,  1.01834446e+00],\n",
              "        [-1.23365878e+00, -8.34049269e-01, -1.00749414e-01]],\n",
              "\n",
              "       [[-8.83499618e-01,  8.07905106e-01, -8.40443570e-01],\n",
              "        [ 1.52142291e-01,  8.73707387e-01,  7.10975062e-01],\n",
              "        [-4.63711781e-01,  1.47875562e+00,  3.55472489e-01],\n",
              "        [ 1.16865191e+00,  1.41691978e+00, -2.90169013e-01],\n",
              "        [-1.42081733e+00, -2.37922695e+00, -1.13329927e+00],\n",
              "        [-7.04762421e-01,  7.56652961e-01,  8.08222216e-01],\n",
              "        [ 1.85749741e-01, -9.26938421e-01,  3.99074641e-01]]]), edge_features=array([[[ 1.11320191,  0.31319528],\n",
              "        [-1.84234498, -0.1997935 ],\n",
              "        [ 1.31342091, -0.40764278],\n",
              "        [ 0.01457634,  0.71471317],\n",
              "        [-0.86307492, -1.35038624],\n",
              "        [-1.63017201,  0.50799828],\n",
              "        [ 1.16804444,  0.58421155]],\n",
              "\n",
              "       [[ 0.77268075, -0.38847858],\n",
              "        [ 1.48165362,  0.72987991],\n",
              "        [-1.26484289,  0.2868988 ],\n",
              "        [-1.54816202, -0.98926897],\n",
              "        [-1.8672202 ,  1.06929901],\n",
              "        [-1.09986711, -0.26372537],\n",
              "        [-0.05963768, -1.37346807]],\n",
              "\n",
              "       [[ 0.62618608, -0.53458533],\n",
              "        [-0.5461073 , -1.12948585],\n",
              "        [-1.57210399, -0.136577  ],\n",
              "        [ 0.27321538, -0.6141448 ],\n",
              "        [ 0.44272056,  2.26628924],\n",
              "        [-0.97375398,  0.50417895],\n",
              "        [ 0.78361166,  0.78828586]],\n",
              "\n",
              "       [[ 2.14426871, -1.01622352],\n",
              "        [-0.97004024,  0.02588691],\n",
              "        [-0.00732849,  0.45321845],\n",
              "        [ 0.57410642, -1.75806857],\n",
              "        [-1.45342333,  1.33335095],\n",
              "        [-0.20220328,  2.30216954],\n",
              "        [ 1.0165604 ,  0.25320211]],\n",
              "\n",
              "       [[-0.53798335,  1.60551642],\n",
              "        [ 2.30853644,  0.40768373],\n",
              "        [-2.26651929,  0.36035641],\n",
              "        [-1.38242325,  0.69597654],\n",
              "        [ 1.19887414, -0.01251907],\n",
              "        [ 0.36591636,  1.3008311 ],\n",
              "        [-0.6695389 , -0.68112435]],\n",
              "\n",
              "       [[ 0.46199636,  0.69027101],\n",
              "        [ 0.06392127, -0.62292991],\n",
              "        [ 0.73480523, -0.53068021],\n",
              "        [-0.79376241,  0.28144688],\n",
              "        [-0.11708044,  0.37442885],\n",
              "        [-1.3862367 ,  2.75113726],\n",
              "        [ 1.25814711, -0.32449819]]]), graph_idx=array([0, 0, 0, 1, 1, 1, 2, 2, 2], dtype=int32), n_graphs=3, node_labels=array([[-1.02967926],\n",
              "       [-0.19428547],\n",
              "       [ 0.63308444],\n",
              "       [ 0.73200377],\n",
              "       [-0.2585156 ],\n",
              "       [ 0.00663284],\n",
              "       [-0.96018829],\n",
              "       [ 0.81866531],\n",
              "       [-0.16067657]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmpSpWtbMpyC"
      },
      "source": [
        "## Random spatio-temporal dataset\n",
        "Here we show the steps to generate the spatio-temporal dataset for ST-GNN training. The assumptions are:\n",
        "* we have a dataset with temporal observations on each nodes at a certain frequency, e.g. daily, weekly, etc.\n",
        "* we have a dataset with temporal observations on each connectivity(directed edge bewteen nodes) at the same frequency with node level observations.\n",
        "\n",
        "This part is just an illustration of how to generate GraphDataset from a spatio-temporal input, the data itself does not carry any meanings or spatio-temporal interations. With this example, it is easy to replace the above random input with realistic spatio-temporal dataset with the same format to run training. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWGowGuZNn3U"
      },
      "source": [
        "### Create node and index mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOJ2KgHsNtum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c2dc5e8-9470-4d7d-eef4-fb0c7442be1a"
      },
      "source": [
        "node_names_idxs = [(chr(i), i-97) for i in range(97, 123)]\n",
        "\n",
        "node_idx2name, node_name2idx = {}, {}\n",
        "for name, idx in node_names_idxs:\n",
        "  node_name2idx[name] = idx\n",
        "  node_idx2name[idx] = name\n",
        "\n",
        "num_nodes = len(node_names_idxs)\n",
        "node_names = node_name2idx.keys()\n",
        "print(f'Total {num_nodes} nodes.')\n",
        "print(f'The nodes are {node_names}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total 26 nodes.\n",
            "The nodes are dict_keys(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cbZQQEkYUzB"
      },
      "source": [
        "### Create node level data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "k89rxsqVPl9K",
        "outputId": "7594c5da-9ab2-40ed-d3e3-1d44de2ad662"
      },
      "source": [
        "num_days = 200\n",
        "year = 2020\n",
        "days = [dayi for dayi in range(num_days)]\n",
        "dates = [(datetime.datetime(year, 1, 1) + datetime.timedelta(dayi)).date().strftime('%Y-%m-%d') for dayi in range(num_days)]\n",
        "\n",
        "# Create node level daily observations\n",
        "# date  day node_name\tfeat1\tfeat2\n",
        "nodes_history = []\n",
        "num_node_feats = 2\n",
        "node_data = None\n",
        "for i in range(num_nodes):\n",
        "  nodes_data_array = np.random.normal(size=[num_days, num_node_feats])\n",
        "  curr_df = pd.DataFrame({'date': dates,\n",
        "                          'day': days,\n",
        "                          'node_idx': i,\n",
        "                          'node_name': node_idx2name[i],\n",
        "                          'node_feat1': nodes_data_array[:, 0],\n",
        "                          'node_feat2': nodes_data_array[:, 1]})\n",
        "  if node_data is None:\n",
        "    node_data = curr_df.copy()\n",
        "  else:\n",
        "    node_data = pd.concat([node_data, curr_df], axis=0)\n",
        "\n",
        "print('A sample of node data: ')\n",
        "node_data.head(5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A sample of node data: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>day</th>\n",
              "      <th>node_idx</th>\n",
              "      <th>node_name</th>\n",
              "      <th>node_feat1</th>\n",
              "      <th>node_feat2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>-1.177029</td>\n",
              "      <td>0.398712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-01-02</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>-3.063158</td>\n",
              "      <td>0.405811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-01-03</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>2.082045</td>\n",
              "      <td>0.675790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-01-04</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>-0.806469</td>\n",
              "      <td>-0.953733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-01-05</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>-0.593832</td>\n",
              "      <td>0.856214</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         date  day  node_idx node_name  node_feat1  node_feat2\n",
              "0  2020-01-01    0         0         a   -1.177029    0.398712\n",
              "1  2020-01-02    1         0         a   -3.063158    0.405811\n",
              "2  2020-01-03    2         0         a    2.082045    0.675790\n",
              "3  2020-01-04    3         0         a   -0.806469   -0.953733\n",
              "4  2020-01-05    4         0         a   -0.593832    0.856214"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EDQnob6YWuh"
      },
      "source": [
        "### Create edge level data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "wuIMjWZgTI2f",
        "outputId": "fd5956e0-5125-4752-e5c7-c8963d8b710e"
      },
      "source": [
        "# Create edge level daily observations\n",
        "# date day src dest\tedge_feat1 edge_feat2\n",
        "\n",
        "# define the range of edge numbers in a graph\n",
        "min_edges, max_edges = 10, 40\n",
        "num_edge_feats = 2\n",
        "\n",
        "# non self-loop candidates\n",
        "node_ids = node_idx2name.keys()\n",
        "pairs = list(itertools.combinations(node_ids, 2))\n",
        "random.shuffle(pairs)\n",
        "total_pairs = len(pairs)\n",
        "\n",
        "\n",
        "edge_data = None\n",
        "for i in range(num_days):\n",
        "  # num of non self-loops edges\n",
        "  n_edges = np.random.randint(min_edges, max_edges+1)\n",
        "  # get n_edges random unique pairs\n",
        "  edge_idxs = random.sample(pairs, n_edges)\n",
        "  edge_src = [e_idx[0] for e_idx in edge_idxs]\n",
        "  edge_dest = [e_idx[1] for e_idx in edge_idxs]\n",
        "  edge_src_name = [node_idx2name[node_i] for node_i in edge_src]\n",
        "  edge_dest_name = [node_idx2name[node_i] for node_i in edge_dest]\n",
        "\n",
        "  edge_data_array = np.random.normal(size=[n_edges, num_edge_feats])\n",
        "  curr_df = pd.DataFrame({'date': (datetime.datetime(year, 1, 1) + datetime.timedelta(i)).date().strftime('%Y-%m-%d'),\n",
        "                          'day': i,\n",
        "                          'src': edge_src,\n",
        "                          'dest': edge_dest,\n",
        "                          'src_name': edge_src_name,\n",
        "                          'dest_name': edge_dest_name,                          \n",
        "                          'edge_feat1': edge_data_array[:, 0],\n",
        "                          'edge_feat2': edge_data_array[:, 1]})\n",
        "  if edge_data is None:\n",
        "    edge_data = curr_df.copy()\n",
        "  else:\n",
        "    edge_data = pd.concat([edge_data, curr_df], axis=0)\n",
        "\n",
        "print('A sample of edge data:')\n",
        "edge_data.head(5)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A sample of edge data:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>day</th>\n",
              "      <th>src</th>\n",
              "      <th>dest</th>\n",
              "      <th>src_name</th>\n",
              "      <th>dest_name</th>\n",
              "      <th>edge_feat1</th>\n",
              "      <th>edge_feat2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>21</td>\n",
              "      <td>o</td>\n",
              "      <td>v</td>\n",
              "      <td>0.167776</td>\n",
              "      <td>2.405669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>15</td>\n",
              "      <td>g</td>\n",
              "      <td>p</td>\n",
              "      <td>0.317989</td>\n",
              "      <td>1.780007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>17</td>\n",
              "      <td>d</td>\n",
              "      <td>r</td>\n",
              "      <td>0.557793</td>\n",
              "      <td>-1.235262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>25</td>\n",
              "      <td>q</td>\n",
              "      <td>z</td>\n",
              "      <td>-0.180008</td>\n",
              "      <td>-0.612154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>14</td>\n",
              "      <td>h</td>\n",
              "      <td>o</td>\n",
              "      <td>-0.910204</td>\n",
              "      <td>0.202476</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         date  day  src  dest src_name dest_name  edge_feat1  edge_feat2\n",
              "0  2020-01-01    0   14    21        o         v    0.167776    2.405669\n",
              "1  2020-01-01    0    6    15        g         p    0.317989    1.780007\n",
              "2  2020-01-01    0    3    17        d         r    0.557793   -1.235262\n",
              "3  2020-01-01    0   16    25        q         z   -0.180008   -0.612154\n",
              "4  2020-01-01    0    7    14        h         o   -0.910204    0.202476"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBgdnlJDYgRf"
      },
      "source": [
        "## Create GraphStDataset from the random data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT-r2beiYoMm"
      },
      "source": [
        "### Implement the GraphSTDataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkBHpo_MYlcK"
      },
      "source": [
        "class GraphStDataset():\n",
        "  \"\"\"Graph spatio-temporal dataset for node level predictions.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               node_history_len,\n",
        "               node_target_len,\n",
        "               edge_history_len,\n",
        "               node_idx2name, \n",
        "               node_name2idx,\n",
        "               node_data,\n",
        "               edge_data,\n",
        "               day_threshold):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      node_history_len: int. The length of past node observations to include.\n",
        "      node_target_len: int. The prediction is node_target_len away.\n",
        "      edge_history_len: int. The length of past edge observations to include.\n",
        "      node_idx2name: a dict. A dict that maps node index to its name.\n",
        "      node_name2idx: a dict. A dict that maps node name to its index.\n",
        "      node_data: a pandas DataFrame. With columns as day, date, node_name, \n",
        "        node_feat1, node_feat2.\n",
        "      day_threshold: int. The cutoff day to use to compute statistics, so we \n",
        "        can normalize the column values.\n",
        "\n",
        "    \"\"\"\n",
        "    self._node_history_len = node_history_len\n",
        "    self._edge_history_len = edge_history_len\n",
        "    self._node_target_len = node_target_len\n",
        "    self._node_idx2name = node_idx2name\n",
        "    self._node_name2idx = node_name2idx\n",
        "    self._node_data = node_data\n",
        "    self._edge_data = edge_data\n",
        "    self._node_data_dict = self._build_node_data_dict(node_data, day_threshold)\n",
        "\n",
        "  def _scale_column_val(self, df, col_name, day_threshold):\n",
        "    min_val = df.loc[df['day']<=day_threshold][col_name].min()\n",
        "    max_val = df.loc[df['day']<=day_threshold][col_name].max()\n",
        "    df['scaled_'+col_name] = (df[col_name]-min_val)/(max_val-min_val)  \n",
        "    return df, min_val, max_val\n",
        "\n",
        "  def _build_node_data_dict(self, node_data, day_threshold):\n",
        "    \"\"\"Processes and builds a dict that maps node_name to its temporal \n",
        "      observations. Also normalize from avaiable observations before threshold.\n",
        "      We can add other derived features, e.g. delta of a certain feature.\n",
        "\n",
        "    Args:\n",
        "      node_data: a pandas DataFrame. With columns as day, date, node_name, \n",
        "        node_feat1, node_feat2.\n",
        "      day_threshold: int. The cutoff day to use to compute statistics, so we \n",
        "        can normalize the column values.\n",
        "\n",
        "    Returns:\n",
        "      node_data_dict: a dict. That maps node_name to its temporal observations.\n",
        "      node_data_scaling_metrics: a dict. That stores the scaling metrics for \n",
        "        each node.\n",
        "    \"\"\"\n",
        "    node_data_dict = {}\n",
        "    self._node_data_scaling_metrics = {}\n",
        "    for node in node_data.node_name.unique():\n",
        "      curr_node_data = node_data.loc[node_data['node_name']==node].copy()\n",
        "      curr_node_data.sort_values(by=['day'], inplace=True)\n",
        "\n",
        "      curr_node_data['node_feat1_delta'] = curr_node_data.node_feat1.diff()\n",
        "      curr_node_data['node_feat2_delta'] = curr_node_data.node_feat1.diff()\n",
        "\n",
        "      curr_node_data, min_val, max_val = self._scale_column_val(curr_node_data, \n",
        "                                                          'node_feat1_delta', \n",
        "                                                          day_threshold)\n",
        "      self._node_data_scaling_metrics[node+'node_feat1_delta']=(min_val, max_val)\n",
        "\n",
        "      curr_node_data, min_val, max_val = self._scale_column_val(curr_node_data, \n",
        "                                                          'node_feat2_delta', \n",
        "                                                          day_threshold)\n",
        "      self._node_data_scaling_metrics[node+'node_feat2_delta']=(min_val, max_val)\n",
        "\n",
        "      # Removes the first row, since it has NaN for deltas\n",
        "      node_data_dict[node] = curr_node_data[1:].copy()\n",
        "    return node_data_dict\n",
        "\n",
        "  def _get_graph(self, \n",
        "                 day_number,\n",
        "                 node_feat_names, \n",
        "                 edge_feat_names, \n",
        "                 node_label_names):\n",
        "    \"\"\"Generates one graph.\n",
        "\n",
        "    Args:\n",
        "      day_number: an int. On which day the graph is built upon. The resulting\n",
        "        graph should have historical observations <= day_number, and lable\n",
        "        should be future observations > day_number.\n",
        "      node_feat_names: a list of str. Node features to be included. \n",
        "      edge_feat_names: a list of str. Edgge features to be included. \n",
        "      node_label_names: a list of str. Node columns to be included as labels. \n",
        "    \"\"\"\n",
        "\n",
        "    # seq_len, node_feat_dim, edge_feat_dim = 7, 3, 2\n",
        "    node_data_dict = self._node_data_dict\n",
        "    node_names = list(node_data_dict.keys())\n",
        "    n_nodes = len(node_names)\n",
        "\n",
        "    node_features = np.zeros((n_nodes, self._node_history_len, len(node_feat_names)))\n",
        "    node_targets = np.zeros((n_nodes, 1, len(node_label_names)))\n",
        "\n",
        "    # Processes node    \n",
        "    for i in range(n_nodes):\n",
        "      curr_data = node_data_dict[node_idx2name[i]].copy()\n",
        "      hist_data = curr_data.loc[(curr_data['day']<=day_number) & \n",
        "                                (curr_data['day']>day_number-self._node_history_len)].copy().sort_values(by=['day'], inplace=False)\n",
        "      time_df = pd.DataFrame({'day': \n",
        "                              list(range(day_number-self._node_history_len+1, day_number+1))})\n",
        "      hist_data = pd.merge(time_df, hist_data, left_on='day', right_on='day', how='left')\n",
        "      hist_data.ffill(inplace=True)\n",
        "\n",
        "      # hist_data\n",
        "      hist_data.fillna(0, inplace=True)\n",
        "      if hist_data.shape[0]>0:\n",
        "        node_features[i, :, :] = hist_data[node_feat_names].values\n",
        "      else: \n",
        "        node_features[i, :, :] = np.zeros(shape=[self._node_history_len, \n",
        "                                          len(node_feat_names)])\n",
        "        \n",
        "      # get delta between curr+target_len and curr\n",
        "      forecast_data = curr_data.loc[(curr_data['day']>=day_number) & \n",
        "                                (curr_data['day']<=day_number+self._node_target_len)].copy().sort_values(by=['day'], inplace=False)\n",
        "      time_df = pd.DataFrame({'day': \n",
        "                              list(range(day_number, day_number+self._node_target_len+1))})\n",
        "      forecast_data = pd.merge(time_df, forecast_data, left_on='day', right_on='day', how='left')\n",
        "      forecast_data.ffill(inplace=True)\n",
        "\n",
        "      # forecast_data\n",
        "      forecast_data.fillna(0, inplace=True)\n",
        "      if forecast_data.shape[0]>0:\n",
        "        # just get the quantity on (t+n)\n",
        "        curr_df = forecast_data[node_label_names].copy()\n",
        "        cu_val = curr_df.loc[curr_df.shape[0]-1]\n",
        "        node_targets[i, :, :] = np.array([cu_val.values]) \n",
        "      else: \n",
        "        # uses zeros as unknowns\n",
        "        node_targets[i, :, :] = np.zeros(shape=[1, len(node_label_names)])\n",
        "        \n",
        "    nodes = np.asarray(node_features)\n",
        "    node_labels = np.asarray(node_targets)\n",
        "\n",
        "    # Processes edge\n",
        "    # each day_number, find all unique pairs in the past history\n",
        "    # and construct edge features.\n",
        "    edge_data_for_day_range = edge_data.loc[(edge_data['day']<=day_number) &\n",
        "                                            (edge_data['day']>day_number-self._edge_history_len)].copy()\n",
        "    unique_edge_pairs = edge_data_for_day_range[['src', 'dest']].copy().drop_duplicates()\n",
        "\n",
        "    edge_features = np.zeros((unique_edge_pairs.shape[0], \n",
        "                              self._edge_history_len, \n",
        "                              len(edge_feat_names)))\n",
        "    from_idx = []\n",
        "    to_idx = []\n",
        "    for idx, row in unique_edge_pairs.iterrows():\n",
        "      src, dest = row.src, row.dest\n",
        "      # add receiver and sender\n",
        "      from_idx.append(src)\n",
        "      to_idx.append(dest)\n",
        "\n",
        "      # add edge features\n",
        "      curr_edge_data = edge_data.loc[(edge_data['day']>day_number-self._edge_history_len) & \n",
        "                                (edge_data['day']<=day_number)&\n",
        "                                (edge_data['src']==src)&\n",
        "                                (edge_data['dest']==dest)].copy().sort_values(by=['day'], inplace=False)\n",
        "      time_df = pd.DataFrame({'day': \n",
        "                              list(range(day_number-self._edge_history_len+1, day_number+1))})   \n",
        "      curr_edge_data = pd.merge(time_df, curr_edge_data, left_on='day', right_on='day', how='left')\n",
        "      curr_edge_data.ffill(inplace=True)\n",
        "\n",
        "      # hist_edge_data\n",
        "      curr_edge_data.fillna(0, inplace=True)\n",
        "      if curr_edge_data.shape[0]>0:\n",
        "        edge_features[i, :, :] = curr_edge_data[edge_feat_names].values\n",
        "      else: \n",
        "        edge_features[i, :, :] = np.zeros(shape=[self._edge_history_len, len(edge_feat_names)])\n",
        "\n",
        "    edges = np.asarray(edge_features)\n",
        "    senders = np.asarray(from_idx) \n",
        "    receivers = np.asarray(to_idx)  \n",
        "\n",
        "    return GraphData(from_idx=senders,\n",
        "                     to_idx=receivers,\n",
        "                     node_features=nodes,\n",
        "                     edge_features=edges,\n",
        "                     node_labels=node_labels,\n",
        "                     graph_idx=np.ones(n_nodes, dtype=np.int32)*day_number,\n",
        "                     n_graphs=1)\n",
        "    \n",
        "  def build_batches(self, node_feat_names, edge_feat_names, node_label_names,\n",
        "                    min_day, max_day, batch_size, ):\n",
        "    \"\"\"Generates batches of graphs.\n",
        "\n",
        "    Args:\n",
        "      min_day, max_day: integers. The date range to build graphs, one graph for\n",
        "        each day.\n",
        "    \"\"\"\n",
        "    days = range(min_day, max_day+1, batch_size)\n",
        "    batched_data = [] # len=total_num_batches\n",
        "    for d in days:\n",
        "      one_batch = []\n",
        "      for curr_d in range(d, d+batch_size):\n",
        "        if curr_d<=max_day:\n",
        "          one_batch.append(self._get_graph(curr_d, \n",
        "                                           node_feat_names, \n",
        "                                           edge_feat_names, \n",
        "                                           node_label_names))\n",
        "\n",
        "      batched_data.append(self._pack_batch(one_batch))\n",
        "    return batched_data\n",
        "\n",
        "  def _pack_batch(self, one_batch):\n",
        "    from_idx = []\n",
        "    to_idx = []\n",
        "    graph_idx = []\n",
        "    node_feat = []\n",
        "    edge_feat = []\n",
        "    node_label =[]\n",
        "\n",
        "    n_total_nodes = 0\n",
        "    n_total_edges = 0\n",
        "    for g in one_batch:\n",
        "      n_nodes = g.node_features.shape[0]\n",
        "      n_edges = g.edge_features.shape[0]\n",
        "      # shift the node indices for the edges\n",
        "      from_idx.append(g.from_idx + n_total_nodes)\n",
        "      to_idx.append(g.to_idx + n_total_nodes)\n",
        "      graph_idx.append(g.graph_idx)\n",
        "      node_feat.append(g.node_features)\n",
        "      edge_feat.append(g.edge_features)\n",
        "      node_label.append(g.node_labels)\n",
        "\n",
        "      n_total_nodes += n_nodes\n",
        "      n_total_edges += n_edges\n",
        "\n",
        "    return GraphData(\n",
        "        from_idx=np.concatenate(from_idx, axis=0),\n",
        "        to_idx=np.concatenate(to_idx, axis=0),\n",
        "        # this task only cares about the structures, the graphs have no features\n",
        "        node_features=np.concatenate(node_feat, axis=0),\n",
        "        edge_features=np.concatenate(edge_feat, axis=0),\n",
        "        graph_idx=np.concatenate(graph_idx, axis=0),\n",
        "        node_labels=np.concatenate(node_label, axis=0),\n",
        "        n_graphs=len(one_batch))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf_ao60Qabrp"
      },
      "source": [
        "### Test the data generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lYnbfy9afxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b3b3df3-ac7e-4cd9-c09d-10039eace2c1"
      },
      "source": [
        "# test generating one graph\n",
        "graph_st_gen = GraphStDataset(node_history_len=7, \n",
        "                              edge_history_len=7,  \n",
        "                              node_target_len=5, \n",
        "                              day_threshold=50,\n",
        "                              node_idx2name=node_idx2name, \n",
        "                              node_name2idx=node_name2idx,\n",
        "                              node_data=node_data, \n",
        "                              edge_data=edge_data)\n",
        "\n",
        "node_feat_names=['node_feat1_delta', 'node_feat2_delta']\n",
        "edge_feat_names=['edge_feat1', 'edge_feat2']\n",
        "lable_names=['node_feat1_delta']\n",
        "min_day, max_day=50, 60\n",
        "batch_size=5\n",
        "test_batch = graph_st_gen.build_batches(node_feat_names,  \n",
        "                                        edge_feat_names, \n",
        "                                        lable_names,\n",
        "                                        min_day, max_day, batch_size)\n",
        "\n",
        "assert test_batch[1].node_features.shape[2]==len(node_feat_names)\n",
        "test_batch[1].node_features.shape\n",
        "test_batch[1].node_labels.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(130, 7, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(130, 1, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbFixOEzOstE"
      },
      "source": [
        "# Build the model\n",
        "\n",
        "To build the model, we need:\n",
        "\n",
        "* Set up the placeholders, default model configuration.\n",
        "* Build the model, build the computation graphs for training and eval, build the metrics and statistics to monitor.\n",
        "* The graphs are batched as a sequence of graphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgJllm6YRczx"
      },
      "source": [
        "### Placeholders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjPcgT_OOuLX"
      },
      "source": [
        "def build_placeholders(node_feature_dim, edge_feature_dim, label_dim,\n",
        "                       node_seq_len, edge_seq_len, label_seq_len):\n",
        "  \"\"\"Builds the placeholders for the model.\n",
        "\n",
        "  Args:\n",
        "    node_feature_dim: int.\n",
        "    edge_feature_dim: int.\n",
        "\n",
        "  Returns:\n",
        "    placeholders: a placeholder name -> placeholder tensor dict.\n",
        "  \"\"\"\n",
        "  return {\n",
        "      'node_features': tf.placeholder(tf.float32, [None, node_seq_len, node_feature_dim]),\n",
        "      'edge_features': tf.placeholder(tf.float32, [None, edge_seq_len, edge_feature_dim]),\n",
        "      'from_idx': tf.placeholder(tf.int32, [None]),\n",
        "      'to_idx': tf.placeholder(tf.int32, [None]),\n",
        "      'graph_idx': tf.placeholder(tf.int32, [None]),\n",
        "      'labels': tf.placeholder(tf.float32, [None, 1, label_dim])\n",
        "      }\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyGG2WySRUtO"
      },
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ro47xcVRWPt"
      },
      "source": [
        "def get_default_config():\n",
        "  \"\"\"The default configs. Make updates accordingly for the learning task.\"\"\"\n",
        "\n",
        "  ###!!! Note to update this configuration!!!#########\n",
        "  day_threshold = 50\n",
        "  # Past history info to use\n",
        "  node_seq_len = 14\n",
        "  edge_seq_len = 14\n",
        "  # Predict for horizon steps away\n",
        "  horizon = 7 \n",
        "\n",
        "  # Control the day range and batch size to use as train and test datasets\n",
        "  # An example is like following:\n",
        "  min_day_train=60  + node_seq_len #60\n",
        "  max_day_train=126 - horizon -1\n",
        "  batch_size_train=32\n",
        "\n",
        "  min_day_test=126 - horizon\n",
        "  max_day_test=150 - horizon  \n",
        "  batch_size_test=5  \n",
        "  ###################################################\n",
        "  label_seq_len=horizon\n",
        "  label_dim=1\n",
        "\n",
        "  node_state_dim = 16\n",
        "  edge_state_dim = 16\n",
        "  graph_rep_dim = 16\n",
        "\n",
        "  graph_st_config = dict(\n",
        "      node_state_dim=node_state_dim,\n",
        "      edge_hidden_sizes=[node_state_dim*2, node_state_dim],\n",
        "      node_hidden_sizes=[node_state_dim],\n",
        "      label_seq_len = label_seq_len,\n",
        "      label_dim=label_dim,\n",
        "      n_prop_layers=6,\n",
        "      # set to False to not share parameters across message passing layers\n",
        "      share_prop_params=True,\n",
        "      # initialize message MLP with small parameter weights to prevent\n",
        "      # aggregated message vectors blowing up, alternatively we could also use\n",
        "      # e.g. layer normalization to keep the scale of these under control.\n",
        "      edge_net_init_scale=0.1,\n",
        "      # other types of update like `mlp` and `residual` can also be used here.\n",
        "      node_update_type='gru',\n",
        "      # set to False if your graph already contains edges in both directions.\n",
        "      use_reverse_direction=False,\n",
        "      reverse_dir_param_different=False,\n",
        "      aggregation_type='mean',\n",
        "      layer_norm=True)\n",
        "\n",
        "  return dict(\n",
        "      data_param=dict(node_seq_len=node_seq_len, \n",
        "                      edge_seq_len=edge_seq_len,\n",
        "                      label_seq_len=label_seq_len,\n",
        "                      label_dim=label_dim,\n",
        "                      day_threshold=day_threshold,\n",
        "                      horizon=horizon),\n",
        "      encoder=dict(\n",
        "          node_hidden_sizes=[node_state_dim],\n",
        "          edge_hidden_sizes=[edge_state_dim]),\n",
        "      output=dict(\n",
        "          node_hidden_sizes=[graph_rep_dim],\n",
        "          gated=True,\n",
        "          output_dim=1),\n",
        "      graph_st_net=graph_st_config,\n",
        "      training=dict(\n",
        "          min_day=min_day_train,\n",
        "          max_day=max_day_train,\n",
        "          batch_size=batch_size_train,\n",
        "          learning_rate=1e-2,\n",
        "          loss='mse',\n",
        "          graph_vec_regularizer_weight=1e-6,\n",
        "          clip_value=10.0,\n",
        "          n_training_steps=10000,\n",
        "          print_after=1,\n",
        "          eval_after=1),\n",
        "      evaluation=dict(\n",
        "          min_day=min_day_test,\n",
        "          max_day=max_day_test,\n",
        "          batch_size=batch_size_test),\n",
        "      seed=8,\n",
        "      )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hc1Uh87RXJV"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iSivSDFRUBl"
      },
      "source": [
        "def build_model(config, node_feature_dim, edge_feature_dim, label_dim):\n",
        "  \"\"\"Create model for training and evaluation.\n",
        "\n",
        "  Args:\n",
        "    config: a dict of configs, like the one created by the `get_default_config`.\n",
        "    node_feature_dim: int, dimension of node features.\n",
        "    edge_feature_dim: int, dimension of edge features.\n",
        "    label_dim: int, dimension of node level labels.\n",
        "\n",
        "  Returns:\n",
        "    tensors: a (potentially nested) name => tensor dict.\n",
        "    placeholders: a (potentially nested) name => tensor dict.\n",
        "    model: a GraphSTNet instance.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if the specified model or training settings are not supported.\n",
        "  \"\"\"\n",
        "  encoder = GraphEncoder(**config['encoder'])\n",
        "  print('Done with encoder.')\n",
        "  output_module = GraphOutput(**config['output'])\n",
        "\n",
        "  model = GraphSTNet(\n",
        "      encoder, \n",
        "      output_module, \n",
        "      **config['graph_st_net'])\n",
        "  print('Done with GraphSTNet.')\n",
        "\n",
        "  training_n_graphs_in_batch = config['training']['batch_size']\n",
        "\n",
        "  node_seq_len = config['data_param']['node_seq_len']\n",
        "  edge_seq_len = config['data_param']['node_seq_len']\n",
        "  label_seq_len = config['data_param']['label_seq_len']\n",
        "  placeholders = build_placeholders(node_feature_dim, \n",
        "                                    edge_feature_dim, \n",
        "                                    label_dim,\n",
        "                                    node_seq_len,\n",
        "                                    edge_seq_len,\n",
        "                                    1)\n",
        "\n",
        "  # training\n",
        "  model_inputs = placeholders.copy()\n",
        "  del model_inputs['labels']\n",
        "  model_inputs['n_graphs'] = training_n_graphs_in_batch\n",
        "\n",
        "  print('Model_inputs: {}'.format(model_inputs))\n",
        "\n",
        "  # run model on the input, should have nodel level prediction\n",
        "  train_pred = model(**model_inputs)  \n",
        "  # print('graph_vector output: {}\\n'.format(graph_vectors))\n",
        "  loss = compute_loss(train_pred, placeholders['labels'],\n",
        "                      loss_type=config['training']['loss'])\n",
        "\n",
        "  optimizer = tf.train.AdamOptimizer(\n",
        "      learning_rate=config['training']['learning_rate'])\n",
        "  grads_and_params = optimizer.compute_gradients(loss)\n",
        "  grads, params = zip(*grads_and_params)\n",
        "  grads, _ = tf.clip_by_global_norm(grads, config['training']['clip_value'])\n",
        "  train_step = optimizer.apply_gradients(zip(grads, params))\n",
        "\n",
        "  grad_scale = tf.global_norm(grads)\n",
        "  param_scale = tf.global_norm(params)\n",
        "\n",
        "  print('Done with optimizer and model.')\n",
        "  # evaluation\n",
        "  model_inputs['n_graphs'] = config['evaluation']['batch_size'] \n",
        "  eval_pred = model(**model_inputs)\n",
        "  mse_loss = compute_loss(eval_pred,\n",
        "                          placeholders['labels'],\n",
        "                          loss_type=config['training']['loss'])\n",
        "\n",
        "  return {\n",
        "      'train_step': train_step,\n",
        "      'metrics': {\n",
        "          'training': {\n",
        "              'loss': loss,\n",
        "              'grad_scale': grad_scale,\n",
        "              'param_scale': param_scale,\n",
        "          },\n",
        "          'validation': {\n",
        "              'mse_loss': mse_loss,\n",
        "          },\n",
        "      },\n",
        "  }, placeholders, model"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd8BGm0ITXjj"
      },
      "source": [
        "# Define utitlity functions for the training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWDxdmstTakH"
      },
      "source": [
        "def build_datasets(config, node_feat_names, edge_feat_names, node_label_name):\n",
        "  \"\"\"Builds the batched datasets for both training and evaluation.\"\"\"\n",
        "  config = copy.deepcopy(config)\n",
        "  data_param = config['data_param']\n",
        "  node_seq_len = data_param['node_seq_len']\n",
        "  edge_seq_len = data_param['edge_seq_len']\n",
        "  label_seq_len = data_param['label_seq_len']\n",
        "  \n",
        "  graph_st_gen = GraphStDataset(node_history_len=node_seq_len, \n",
        "                                edge_history_len=edge_seq_len,  \n",
        "                                node_target_len=label_seq_len, \n",
        "                                day_threshold=data_param['day_threshold'],\n",
        "                                node_idx2name=node_idx2name, \n",
        "                                node_name2idx=node_name2idx,\n",
        "                                node_data=node_data, \n",
        "                                edge_data=edge_data)\n",
        "\n",
        "  node_feat_names=['node_feat1_delta', 'node_feat2_delta']\n",
        "  edge_feat_names=['edge_feat1', 'edge_feat2']\n",
        "  lable_names=['node_feat1_delta']  \n",
        "  \n",
        "  min_day, max_day = config['training']['min_day'], config['training']['max_day']\n",
        "  batch_size = config['training']['batch_size']\n",
        "  training_set = graph_st_gen.build_batches(node_feat_names, \n",
        "                                            edge_feat_names, \n",
        "                                            node_label_name,\n",
        "                                            min_day, max_day,\n",
        "                                            batch_size\n",
        "                                            )\n",
        "  \n",
        "  min_day, max_day = config['evaluation']['min_day'], config['evaluation']['max_day']\n",
        "  batch_size = config['evaluation']['batch_size']  \n",
        "  validation_set = graph_st_gen.build_batches(node_feat_names, \n",
        "                                              edge_feat_names, \n",
        "                                              node_label_name,\n",
        "                                              min_day, max_day,\n",
        "                                              batch_size)\n",
        "  \n",
        "  return training_set, validation_set, graph_st_gen\n",
        "\n",
        "def fill_feed_dict(placeholders, batch):\n",
        "  \"\"\"Creates a feed dict for the given batch of data.\n",
        "\n",
        "  Args:\n",
        "    placeholders: a dict of placeholders.\n",
        "    batch: a batch of data, should be a `GraphData` instance.\n",
        "\n",
        "  Returns:\n",
        "    feed_dict: a feed_dict that can be used in a session run call.\n",
        "  \"\"\"\n",
        "  if isinstance(batch, GraphData):\n",
        "    graphs = batch\n",
        "    labels = None\n",
        "  else:\n",
        "    graphs, labels = batch  \n",
        "  feed_dict = {\n",
        "      placeholders['node_features']: graphs.node_features,\n",
        "      placeholders['edge_features']: graphs.edge_features,\n",
        "      placeholders['from_idx']: graphs.from_idx,\n",
        "      placeholders['to_idx']: graphs.to_idx,\n",
        "      placeholders['graph_idx']: graphs.graph_idx,\n",
        "      placeholders['labels'] : graphs.node_labels,\n",
        "  }  \n",
        "  return feed_dict\n",
        "\n",
        "def evaluate(sess, eval_metrics, placeholders, validation_set, batch_size):\n",
        "  \"\"\"Evaluates model performance on the given validation set.\n",
        "\n",
        "  Args:\n",
        "    sess: a `tf.Session` instance used to run the computation.\n",
        "    eval_metrics: a dict containing tensor 'mse_loss'.\n",
        "    placeholders: a placeholder dict.\n",
        "    validation_set: a collection of `GraphDataset` instance, calling `pairs` and\n",
        "      `triplets` functions with `batch_size` creates iterators over a finite\n",
        "      sequence of batches to evaluate on.\n",
        "    batch_size: number of batches to use for each session run call.\n",
        "\n",
        "  Returns:\n",
        "    metrics: a dict of metric name => value mapping.\n",
        "  \"\"\"\n",
        "  accumulated_mse_loss = []\n",
        "  # batch=validation_set._get_one_batch(batch_size)\n",
        "\n",
        "  for batch in validation_set:\n",
        "    feed_dict = fill_feed_dict(placeholders, batch)\n",
        "    mse_loss = sess.run(eval_metrics['mse_loss'], feed_dict=feed_dict)\n",
        "    accumulated_mse_loss.append(mse_loss)\n",
        "\n",
        "  return {\n",
        "      'mse_loss': np.mean(accumulated_mse_loss),\n",
        "  }"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5twvd2LUl1R"
      },
      "source": [
        "# Run pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vVV3CEYUoR6"
      },
      "source": [
        "config = get_default_config()\n",
        "\n",
        "config['training']['n_training_steps'] = 5000"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL9SNpbrTfUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83f538af-ea8a-4c28-985a-11d6622d5677"
      },
      "source": [
        "# Run this if you want to run the code again, otherwise tensorflow would\n",
        "# complain that you already created the same graph and the same variables.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Set random seeds\n",
        "seed = config['seed']\n",
        "random.seed(seed)\n",
        "np.random.seed(seed + 1)\n",
        "tf.set_random_seed(seed + 2)\n",
        "\n",
        "\n",
        "node_feat_names=['node_feat1_delta', 'node_feat1_delta']\n",
        "edge_feat_names=['edge_feat1_delta', 'edge_feat2_delta']\n",
        "node_label_name=['node_feat1_delta']\n",
        "\n",
        "training_batch_set, validation_set, graph_st_gen = build_datasets(config, \n",
        "                                              node_feat_names, \n",
        "                                              edge_feat_names, \n",
        "                                              node_label_name)\n",
        "first_batch_graphs = training_batch_set[0]\n",
        "\n",
        "node_feature_dim = first_batch_graphs.node_features.shape[-1]\n",
        "edge_feature_dim = first_batch_graphs.edge_features.shape[-1]\n",
        "label_dim = first_batch_graphs.node_labels.shape[-1]\n",
        "node_feature_dim, edge_feature_dim, label_dim"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 2, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LzdCGklf9bi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e597b45b-888a-4a24-e623-a2ef9b615f02"
      },
      "source": [
        "print('Num of training batches: {}'.format(len(training_batch_set)))\n",
        "print('Num of training batches: {}'.format(len(validation_set)))\n",
        "print('Num of graphs in one training batch: {}'.format(training_batch_set[0].n_graphs))\n",
        "print('Num of graphs in one validation batch: {}'.format(validation_set[0].n_graphs))\n",
        "\n",
        "print('Node data shape in Train: {}'.format(training_batch_set[0].node_features.shape))\n",
        "print('Node label data shape in Train: {}'.format(training_batch_set[0].node_labels.shape))\n",
        "print('Edge data shape in Train: {}'.format(training_batch_set[0].edge_features.shape))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num of training batches: 2\n",
            "Num of training batches: 5\n",
            "Num of graphs in one training batch: 32\n",
            "Num of graphs in one validation batch: 5\n",
            "Node data shape in Train: (832, 14, 2)\n",
            "Node label data shape in Train: (832, 1, 1)\n",
            "Edge data shape in Train: (7240, 14, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo0OUI64vJjq"
      },
      "source": [
        "## Run training loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2gPgwILc0AT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b4b54a3-5ae8-47e5-c1d4-04396cced2df"
      },
      "source": [
        "# del model, placeholders, tensors\n",
        "epochs=10\n",
        "first_batch_graphs = training_batch_set[0]\n",
        "\n",
        "node_feature_dim = first_batch_graphs.node_features.shape[-1]\n",
        "edge_feature_dim = first_batch_graphs.edge_features.shape[-1]\n",
        "label_dim = first_batch_graphs.node_labels.shape[-1]\n",
        "node_feature_dim, edge_feature_dim, label_dim\n",
        "\n",
        "tensors, placeholders, model = build_model(\n",
        "    config, node_feature_dim, edge_feature_dim, label_dim)\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "init_ops = (tf.global_variables_initializer(),\n",
        "            tf.local_variables_initializer())\n",
        "\n",
        "# If there is already a session instance, close it and start a new one\n",
        "if 'sess' in globals():\n",
        "  sess.close()\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(init_ops)\n",
        "\n",
        "print('Done with initialization.')\n",
        "\n",
        "accumulated_metrics = collections.defaultdict(list)\n",
        "\n",
        "early_stop_last_n = 10\n",
        "val_loss_list = []\n",
        "training_done_flag = False\n",
        "for i_epoch in range(epochs):\n",
        "  for i_iter, batch in enumerate(training_batch_set):\n",
        "    _, train_metrics = sess.run(\n",
        "        [tensors['train_step'], tensors['metrics']['training']],\n",
        "        feed_dict=fill_feed_dict(placeholders, batch))\n",
        "\n",
        "    # Accumulate over minibatches to reduce variance in the training metrics\n",
        "    for k, v in train_metrics.items():\n",
        "      accumulated_metrics[k].append(v)\n",
        "\n",
        "    if (i_iter + 1) % config['training']['print_after'] == 0:\n",
        "      metrics_to_print = {\n",
        "          k: np.mean(v) for k, v in accumulated_metrics.items()}\n",
        "      info_str = ', '.join(\n",
        "          ['%s %.4f' % (k, v) for k, v in metrics_to_print.items()])\n",
        "      # reset the metrics\n",
        "      accumulated_metrics = collections.defaultdict(list)\n",
        "\n",
        "      if ((i_iter + 1) // config['training']['print_after'] %\n",
        "          config['training']['eval_after'] == 0):\n",
        "        eval_metrics = evaluate(\n",
        "            sess, tensors['metrics']['validation'], placeholders,\n",
        "            validation_set, config['evaluation']['batch_size'])\n",
        "        info_str += ', ' + ', '.join(\n",
        "            ['%s %.4f' % ('val/' + k, v) for k, v in eval_metrics.items()])\n",
        "        val_loss_list.append(eval_metrics['mse_loss'])\n",
        "\n",
        "      print('epoch %d, iter %d, %s, time %.2fs' % (\n",
        "          i_epoch + 1, i_iter + 1, info_str, time.time() - t_start))\n",
        "      t_start = time.time()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 2, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "stream",
          "text": [
            "Done with encoder.\n",
            "Done with GraphSTNet.\n",
            "Model_inputs: {'node_features': <tf.Tensor 'Placeholder:0' shape=(?, 14, 2) dtype=float32>, 'edge_features': <tf.Tensor 'Placeholder_1:0' shape=(?, 14, 2) dtype=float32>, 'from_idx': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=int32>, 'to_idx': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=int32>, 'graph_idx': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=int32>, 'n_graphs': 32}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/sonnet/python/modules/base.py:278: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/sonnet/python/modules/basic_rnn.py:280: The name tf.logging.log_first_n is deprecated. Please use tf.compat.v1.logging.log_first_n instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/sonnet/python/modules/basic_rnn.py:281: The name tf.logging.WARN is deprecated. Please use tf.compat.v1.logging.WARN instead.\n",
            "\n",
            "WARNING:tensorflow:The `skip_connections` argument will be deprecated.\n",
            "WARNING:tensorflow:From <ipython-input-4-7118068a4f6a>:48: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/sonnet/python/modules/basic.py:126: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/sonnet/python/modules/gated_rnn.py:287: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:The `skip_connections` argument will be deprecated.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/sonnet/python/modules/base.py:579: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/sonnet/python/modules/basic.py:131: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done with optimizer and model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "stream",
          "text": [
            "Done with initialization.\n",
            "epoch 1, iter 1, loss 2.2284, grad_scale 3.5415, param_scale 15.7744, val/mse_loss 1.9194, time 3.77s\n",
            "epoch 1, iter 2, loss 2.0003, grad_scale 2.0718, param_scale 15.7968, val/mse_loss 1.8198, time 0.90s\n",
            "epoch 2, iter 1, loss 2.2030, grad_scale 0.2162, param_scale 15.8158, val/mse_loss 1.8227, time 1.68s\n",
            "epoch 2, iter 2, loss 1.8865, grad_scale 0.2048, param_scale 15.8420, val/mse_loss 1.8210, time 0.89s\n",
            "epoch 3, iter 1, loss 2.2036, grad_scale 0.1722, param_scale 15.8724, val/mse_loss 1.8201, time 1.67s\n",
            "epoch 3, iter 2, loss 1.8844, grad_scale 0.0506, param_scale 15.9053, val/mse_loss 1.8201, time 0.92s\n",
            "epoch 4, iter 1, loss 2.2015, grad_scale 0.0160, param_scale 15.9392, val/mse_loss 1.8202, time 1.68s\n",
            "epoch 4, iter 2, loss 1.8850, grad_scale 0.1001, param_scale 15.9735, val/mse_loss 1.8200, time 0.91s\n",
            "epoch 5, iter 1, loss 2.2011, grad_scale 0.0275, param_scale 16.0072, val/mse_loss 1.8200, time 1.70s\n",
            "epoch 5, iter 2, loss 1.8839, grad_scale 0.0386, param_scale 16.0407, val/mse_loss 1.8202, time 0.90s\n",
            "epoch 6, iter 1, loss 2.2013, grad_scale 0.0938, param_scale 16.0737, val/mse_loss 1.8202, time 1.69s\n",
            "epoch 6, iter 2, loss 1.8831, grad_scale 0.0146, param_scale 16.1061, val/mse_loss 1.8201, time 0.91s\n",
            "epoch 7, iter 1, loss 2.2004, grad_scale 0.0840, param_scale 16.1385, val/mse_loss 1.8199, time 1.71s\n",
            "epoch 7, iter 2, loss 1.8822, grad_scale 0.0539, param_scale 16.1703, val/mse_loss 1.8201, time 0.91s\n",
            "epoch 8, iter 1, loss 2.1989, grad_scale 0.0229, param_scale 16.2028, val/mse_loss 1.8203, time 1.74s\n",
            "epoch 8, iter 2, loss 1.8811, grad_scale 0.0974, param_scale 16.2348, val/mse_loss 1.8205, time 0.95s\n",
            "epoch 9, iter 1, loss 2.1981, grad_scale 0.0223, param_scale 16.2677, val/mse_loss 1.8208, time 1.70s\n",
            "epoch 9, iter 2, loss 1.8788, grad_scale 0.0563, param_scale 16.2995, val/mse_loss 1.8215, time 0.91s\n",
            "epoch 10, iter 1, loss 2.1976, grad_scale 0.0623, param_scale 16.3317, val/mse_loss 1.8223, time 1.70s\n",
            "epoch 10, iter 2, loss 1.8768, grad_scale 0.0378, param_scale 16.3628, val/mse_loss 1.8230, time 0.91s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhDasD3wgK7H"
      },
      "source": [
        "# Check model predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8tIp8z4sXb0"
      },
      "source": [
        "## For Spatio-temporal dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD6Pi_UKlL2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e33cd5b-9d31-45b3-c99a-b98c4ed5c53e"
      },
      "source": [
        "\n",
        "model_inputs = placeholders.copy()\n",
        "del model_inputs['labels']\n",
        "\n",
        "# build dict of lists\n",
        "# day_when_to_make_pred | day_pred | node_name| pred | true\n",
        "from collections import defaultdict\n",
        "\n",
        "res = defaultdict(list)\n",
        "total_nodes=26\n",
        "horizon = config['data_param']['horizon']\n",
        "for iter, val in enumerate(validation_set):\n",
        "  n_graphs = val.n_graphs\n",
        "  print('Eval on iter {}, total graph={}, total nodes={}'.format(iter, n_graphs, len(val.graph_idx)))\n",
        "  \n",
        "  y= model(n_graphs=n_graphs, **model_inputs)\n",
        "  check_mse_loss = compute_loss(y, placeholders['labels'])  \n",
        "  yhat, mse_loss_val = sess.run([y, check_mse_loss],\n",
        "                                feed_dict=fill_feed_dict(placeholders, val))\n",
        "  \n",
        "  for i, day in enumerate(val.graph_idx):\n",
        "    for j_pred in range(1): # we are only predicting for one step\n",
        "      res['day_when_to_make_pred'].append(day)\n",
        "      res['node_name'].append(node_idx2name[i%total_nodes])\n",
        "      res['day_pred'].append(day+horizon+j_pred+1)   # should be on the day we are predicting for\n",
        "      res['true_scaled'].append(val.node_labels[i][j_pred][0])\n",
        "      res['pred_scaled'].append(yhat[i][j_pred])\n",
        "res_df = pd.DataFrame(res)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eval on iter 0, total graph=5, total nodes=130\n",
            "Eval on iter 1, total graph=5, total nodes=130\n",
            "Eval on iter 2, total graph=5, total nodes=130\n",
            "Eval on iter 3, total graph=5, total nodes=130\n",
            "Eval on iter 4, total graph=5, total nodes=130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "RUROf0X5wGO0",
        "outputId": "7dc74ea7-ee52-411b-e129-f347ce3aff32"
      },
      "source": [
        "res_df.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day_when_to_make_pred</th>\n",
              "      <th>node_name</th>\n",
              "      <th>day_pred</th>\n",
              "      <th>true_scaled</th>\n",
              "      <th>pred_scaled</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>119</td>\n",
              "      <td>a</td>\n",
              "      <td>127</td>\n",
              "      <td>-2.195130</td>\n",
              "      <td>0.039446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>119</td>\n",
              "      <td>b</td>\n",
              "      <td>127</td>\n",
              "      <td>1.446449</td>\n",
              "      <td>-0.061275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>119</td>\n",
              "      <td>c</td>\n",
              "      <td>127</td>\n",
              "      <td>1.219629</td>\n",
              "      <td>-0.086640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>119</td>\n",
              "      <td>d</td>\n",
              "      <td>127</td>\n",
              "      <td>-0.443102</td>\n",
              "      <td>-0.096840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>119</td>\n",
              "      <td>e</td>\n",
              "      <td>127</td>\n",
              "      <td>1.045137</td>\n",
              "      <td>-0.110327</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   day_when_to_make_pred node_name  day_pred  true_scaled  pred_scaled\n",
              "0                    119         a       127    -2.195130     0.039446\n",
              "1                    119         b       127     1.446449    -0.061275\n",
              "2                    119         c       127     1.219629    -0.086640\n",
              "3                    119         d       127    -0.443102    -0.096840\n",
              "4                    119         e       127     1.045137    -0.110327"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhKX1ZcE0uPq"
      },
      "source": [
        "## Scale back the predictions and compute metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG3nbFjg45fK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a1bcc2f-0b3c-47dd-a139-dce4011f5f68"
      },
      "source": [
        "import sklearn, scipy, math\n",
        "def compute_rmse(actual, predicted):\n",
        "  mse = sklearn.metrics.mean_squared_error(actual, predicted)\n",
        "  return math.sqrt(mse)\n",
        "\n",
        "def compute_corr(actual, predicted):\n",
        "  return scipy.stats.pearsonr(actual, predicted)[0]\n",
        "\n",
        "def compute_mape(actual, predicted):\n",
        "  y_true, y_pred = np.array(actual)+1, np.array(predicted)\n",
        "  return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  \n",
        "  # return sklearn.metrics.mean_absolute_percentage_error(actual, predicted)\n",
        "\n",
        "def compute_rmsle(actual, predicted):\n",
        "  assert len(actual) == len(predicted)\n",
        "  return np.sqrt(np.mean((np.log(1+predicted) - np.log(1+actual))**2))\n",
        "\n",
        "horizon = config['data_param']['horizon']\n",
        "max_day = config['evaluation']['max_day']\n",
        "n_step_ahead = 1\n",
        "label_name_val = 'node_feat1_delta'\n",
        "\n",
        "node_data_dict = graph_st_gen._node_data_dict\n",
        "node_data_scaling_metrics = graph_st_gen._node_data_scaling_metrics\n",
        "res_metrics = defaultdict(list)\n",
        "all_regions_data = None \n",
        "for region in node_data_dict.keys():\n",
        "  region_data = res_df.loc[(res_df['node_name']==region) & (res_df['day_when_to_make_pred']+horizon+1==res_df['day_pred'])].copy()\n",
        "  region_data.set_index('day_pred', inplace=True)\n",
        "  min_val, max_val =  node_data_scaling_metrics[region+label_name_val]\n",
        "\n",
        "  # new cases per capital\n",
        "  region_data['true'] = region_data['true_scaled']*(max_val-min_val)+min_val\n",
        "  region_data['pred'] = region_data['pred_scaled']*(max_val-min_val)+min_val\n",
        "\n",
        "  # region_data = region_data.loc[region_data['day_when_to_make_pred']<=241-horizon].copy()\n",
        "\n",
        "  def cap_at_zero(val):\n",
        "    if val<0:\n",
        "      return 0.0\n",
        "    else:\n",
        "      return val\n",
        "\n",
        "  region_data['pred'] = region_data.apply(lambda row: cap_at_zero(row['pred']), axis=1)\n",
        "  \n",
        "  data_to_save = region_data[['node_name', \"day_when_to_make_pred\", 'true', 'pred']].copy()\n",
        "  data_to_save['horizon'] = horizon\n",
        "  data_to_save['method'] ='spatiotemporal_gnn'\n",
        "  data_to_save = data_to_save.rename(columns={\"day_when_to_make_pred\": \"day\", \n",
        "                                              \"node_name\": \"state\", \n",
        "                                              \"pred\": \"pred\",\n",
        "                                              \"horizon\": \"horizon\",\n",
        "                                              \"true\": \"gt\", \n",
        "                                              'method': \"method\"}, inplace=False)\n",
        "  \n",
        "  if all_regions_data is None:\n",
        "    all_regions_data = data_to_save.copy()\n",
        "  else:\n",
        "    all_regions_data = all_regions_data.append(data_to_save, ignore_index=True)\n",
        "\n",
        "  rmse_metric = compute_rmse(region_data['true'], region_data['pred'])\n",
        "  res_metrics['region'].append(region)\n",
        "  res_metrics['rmse'].append(rmse_metric)\n",
        "  res_metrics['corr'].append(compute_corr(region_data['true'], region_data['pred']))\n",
        "  res_metrics['mape'].append(compute_mape(region_data['true'], region_data['pred']))\n",
        "  res_metrics['rmsle'].append(compute_rmsle(region_data['true'], region_data['pred']))\n",
        "\n",
        "res_metrics_df = pd.DataFrame(res_metrics)\n",
        "\n",
        "res_metrics_df.mean()\n",
        "\n",
        "import datetime\n",
        "all_regions_data['date'] = all_regions_data.apply(lambda row: (datetime.date(2020, 1, 1) + datetime.timedelta(row.day+horizon)).strftime('%Y-%m-%d'),axis=1)\n",
        "all_regions_data.tail()\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "rmse       8.752330\n",
              "corr            NaN\n",
              "mape     100.000000\n",
              "rmsle      1.725182\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>state</th>\n",
              "      <th>day</th>\n",
              "      <th>gt</th>\n",
              "      <th>pred</th>\n",
              "      <th>horizon</th>\n",
              "      <th>method</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>645</th>\n",
              "      <td>z</td>\n",
              "      <td>139</td>\n",
              "      <td>-10.659490</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "      <td>spatiotemporal_gnn</td>\n",
              "      <td>2020-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>646</th>\n",
              "      <td>z</td>\n",
              "      <td>140</td>\n",
              "      <td>4.449858</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "      <td>spatiotemporal_gnn</td>\n",
              "      <td>2020-05-27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>647</th>\n",
              "      <td>z</td>\n",
              "      <td>141</td>\n",
              "      <td>-7.771705</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "      <td>spatiotemporal_gnn</td>\n",
              "      <td>2020-05-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>648</th>\n",
              "      <td>z</td>\n",
              "      <td>142</td>\n",
              "      <td>-10.059340</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "      <td>spatiotemporal_gnn</td>\n",
              "      <td>2020-05-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649</th>\n",
              "      <td>z</td>\n",
              "      <td>143</td>\n",
              "      <td>-1.487060</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "      <td>spatiotemporal_gnn</td>\n",
              "      <td>2020-05-30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    state  day         gt  pred  horizon              method        date\n",
              "645     z  139 -10.659490   0.0        7  spatiotemporal_gnn  2020-05-26\n",
              "646     z  140   4.449858   0.0        7  spatiotemporal_gnn  2020-05-27\n",
              "647     z  141  -7.771705   0.0        7  spatiotemporal_gnn  2020-05-28\n",
              "648     z  142 -10.059340   0.0        7  spatiotemporal_gnn  2020-05-29\n",
              "649     z  143  -1.487060   0.0        7  spatiotemporal_gnn  2020-05-30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "534vRNPy9gOl"
      },
      "source": [
        "def plot_region(res_df, region, n_day_ahead=7, scaled=False, ax=None):\n",
        "  col_to_view = ['true_scaled','pred_scaled']\n",
        "  region_data = res_df.loc[(res_df['node_name']==region) & (res_df['day_when_to_make_pred']+n_day_ahead+1==res_df['day_pred'])].copy()\n",
        "  region_data.set_index('day_pred', inplace=True)\n",
        "\n",
        "  if scaled==False:\n",
        "    col_to_view = ['true', 'pred']\n",
        "    min_val, max_val =  node_data_scaling_metrics[region+label_name_val]\n",
        "    region_data['true'] = region_data['true_scaled']*(max_val-min_val)+min_val\n",
        "    region_data['pred'] = region_data['pred_scaled']*(max_val-min_val)+min_val\n",
        "\n",
        "  region_data\n",
        "  if ax is None:  \n",
        "    plt.plot(region_data[col_to_view], title=region)\n",
        "  region_data[col_to_view].plot(ax=ax)\n",
        "  ax.set_title(region)\n",
        "  ax.legend()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOlkEXpPqKtE"
      },
      "source": [
        "m, n = 2, 3\n",
        "fig, ax = plt.subplots(m, n, figsize=(15, 11))\n",
        "regions = ['a','b','c','d','e','f']\n",
        "for i in range(len(regions)):\n",
        "  fx, fy = int(i/n), i%n\n",
        "  ax[fx][fy]\n",
        "  plot_region(res_df, regions[i], n_day_ahead=7, scaled=False, ax=ax[fx][fy])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}